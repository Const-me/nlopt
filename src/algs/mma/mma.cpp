/* Copyright (c) 2007-2014 Massachusetts Institute of Technology
 *
 * Permission is hereby granted, free of charge, to any person obtaining
 * a copy of this software and associated documentation files (the
 * "Software"), to deal in the Software without restriction, including
 * without limitation the rights to use, copy, modify, merge, publish,
 * distribute, sublicense, and/or sell copies of the Software, and to
 * permit persons to whom the Software is furnished to do so, subject to
 * the following conditions:
 *
 * The above copyright notice and this permission notice shall be
 * included in all copies or substantial portions of the Software.
 *
 * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
 * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
 * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
 * LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
 * OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
 * WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 */

#include <stdlib.h>
#include <math.h>
#include <string.h>
#include <stdio.h>

#include <assert.h>
#include <stdint.h>
#include <immintrin.h>
#include <algorithm>
#include <memory>
#include <vector>
#include <cmath>
#include <intrin.h>

#include "mma.h"
#include "nlopt-util.h"

// 1 to use parallel version leveraging Windows OS-supplied thread pool
// 0 to use single core version
// Both versions require AVX2 and FMA3 ISA extensions
#define USE_PARALLEL_FOR 1

#if USE_PARALLEL_FOR
#include "../../util/ParallelReduce.h"

namespace
{
	constexpr size_t maxJobsPerThread = 8;
}
#endif

unsigned mma_verbose = 0; /* > 0 for verbose output */

#define MIN(a,b) std::min((a), (b))
#define MAX(a,b) std::max((a), (b))

/* magic minimum value for rho in MMA ... the 2002 paper says it should
   be a "fixed, strictly positive `small' number, e.g. 1e-5"
   ... grrr, I hate these magic numbers, which seem like they
   should depend on the objective function in some way ... in particular,
   note that rho is dimensionful (= dimensions of objective function) */
#define MMA_RHOMIN 1e-5

   /***********************************************************************/
   /* function for MMA's dual solution of the approximate problem */

struct dual_data;

struct dual_data
{
	int count; /* evaluation count, incremented each call */
	unsigned n; /* must be set on input to dimension of x */
	const double* x, * lb, * ub, * sigma, * dfdx; /* arrays of length n */
	const double* dfcdx; /* m-by-n array of fc gradients */
	double fval, rho; /* must be set on input */
	const double* fcval, * rhoc; /* arrays of length m */
	double* xcur; /* array of length n, output each time */
	double gval, wval, * gcval; /* output each time (array length m) */

#if USE_PARALLEL_FOR
	size_t m = 0;
	const double* y = nullptr;
	size_t jobSizeLog2 = 0;
	size_t countJobs = 0;
	ParallelReduce<dual_data> parallelReduce;

	void computeJobSize( size_t elements ) noexcept
	{
		const size_t maxJobs = parallelReduce.countWorkers() * maxJobsPerThread;

		size_t minJobSize = ( elements + maxJobs - 1 ) / maxJobs;
		// Clamp to be >= 1024 elements per job, which also ensures nonzero input to _BitScanReverse64
		minJobSize = std::max( minJobSize, (size_t)1024 );

		// Round minJobSize up to a power of 2, the formula for the logarithm of that is BSR( val - 1 ) + 1
		unsigned long log2;
		_BitScanReverse64( &log2, minJobSize - 1 );
		log2++;

		// Store jobSizeLog2 field, and compute count of these jobs
		jobSizeLog2 = log2;
		const size_t mask = ( size_t( 1 ) << log2 ) - 1;
		countJobs = ( elements + mask ) >> log2;
	}
#endif
};

namespace
{
	// std::isnan( x ) ? false : true
	__forceinline bool isNotNan( double x ) { return x == x; }

	// std::isnan( x ) ? 0.0 : x
	__forceinline double zeroFromNan( double x )
	{
		const __m128d v = _mm_set_sd( x );
		const __m128d cmp = _mm_cmpeq_pd( v, v );
		return _mm_cvtsd_f64( _mm_and_pd( v, cmp ) );
	}

	// std::isnan( x ) ? 0.0 : x for all 4 lanes in the vector
	__forceinline __m256d zeroFromNan( const __m256d v )
	{
		const __m256d cmp = _mm256_cmp_pd( v, v, _CMP_EQ_OQ );
		return _mm256_and_pd( v, cmp );
	}

	// A vector of 4 int64_t integers generated by the following expression:
	// ( ( [ 0, 1, 2, 3 ] + j ) < n ) ? -1 : 0;
	// Note the integer -1 number has all bits set
	__forceinline __m256i remainderLoadMask( size_t j, size_t n )
	{
		// Make a mask of 4 bytes
		// These aren't branches, they should compile to conditional moves
		ptrdiff_t missingLanes = j + 4 - n;
		missingLanes = std::max( missingLanes, (ptrdiff_t)0 );

		uint32_t mask = -( missingLanes < 4 );
		mask >>= missingLanes * 8;
		// Sign extend the bytes into int64 lanes in AVX vector
		__m128i tmp = _mm_cvtsi32_si128( (int)mask );
		return _mm256_cvtepi8_epi64( tmp );
	}

	static const __m256d signBits = _mm256_set1_pd( -0.0 );

	// std::abs( v ) for all 4 lanes in the FP64 vector
	__forceinline __m256d vectorAbs( __m256d v )
	{
		return _mm256_andnot_pd( signBits, v );
	}

	// Negate all lanes in the FP64 vector
	__forceinline __m256d vectorNegate( __m256d v )
	{
		return _mm256_xor_pd( v, signBits );
	}

	// Compute v^2
	__forceinline __m256d vectorSquare( __m256d v )
	{
		return _mm256_mul_pd( v, v );
	}

	// Sum of all 4 FP64 elements in the AVX vector
	__forceinline double horizontalSum( __m256d v4 )
	{
		__m128d v = _mm256_extractf128_pd( v4, 1 );
		v = _mm_add_pd( v, _mm256_castpd256_pd128( v4 ) );
		v = _mm_add_sd( v, _mm_unpackhi_pd( v, v ) );
		return _mm_cvtsd_f64( v );
	}

	// rdi[ 0 .. length - 1 ] = -rsi[ 0 .. length - 1 ]
	inline void negateDoubles( double* rdi, const size_t length, const double* rsi )
	{
		// Round the length down to a multiple of 4
		const size_t lengthAligned = length & (ptrdiff_t)-4;
		const double* const rsiEndAligned = rsi + lengthAligned;

		// Using proper formula *rdi = ( 0.0 - *rsi ) because the output numbers are consumed by external code in this DLL.
		// Also this function bottlenecks on memory stores not compute, the arithmetic instruction is essentially free
		const __m256d zero = _mm256_setzero_pd();

		// Handle full vectors in the loop
		for( ; rsi < rsiEndAligned; rdi += 4 )
		{
			__m256d v = _mm256_loadu_pd( rsi );
			rsi += 4;
			v = _mm256_sub_pd( zero, v );
			_mm256_storeu_pd( rdi, v );
		}

		// Handle the remainder
		const size_t rem = length % 4;
		if( 0 != rem )
		{
			const __m256i loadMask = remainderLoadMask( 0, rem );
			__m256d v = _mm256_maskload_pd( rsi, loadMask );
			v = _mm256_sub_pd( zero, v );
			_mm256_maskstore_pd( rdi, loadMask, v );
		}
	}
}

#if USE_PARALLEL_FOR
static void poolCallback( dual_data* const d, const size_t idxJob, const size_t idxThread, double* const reductionBuffer ) noexcept
{
	const size_t n = d->n;
	const size_t m = d->m;
	const double* x = d->x;
	const double* lb = d->lb;
	const double* ub = d->ub;
	const double* sigma = d->sigma;
	const double* dfdx = d->dfdx;
	const double* dfcdx = d->dfcdx;
	const double* rhoc = d->rhoc;
	const double* fcval = d->fcval;
	const double* y = d->y;

	double* xcur = d->xcur;
	// Use thread local output buffer to accumulate `gcval` numbers, starting from offset 12
	double* const gcval = reductionBuffer + 12;

	const size_t jobSizeLog2 = d->jobSizeLog2;
	const size_t jBegin = idxJob << jobSizeLog2;
	const size_t jEnd = std::min( ( idxJob + 1 ) << jobSizeLog2, n );

	// Create some constant vectors
	const __m256d half = _mm256_set1_pd( 0.5 );
	const __m256d one = _mm256_set1_pd( 1 );
	const __m256d pointNine = _mm256_set1_pd( 0.9 );
	const __m256d halfRho = _mm256_set1_pd( d->rho * _mm256_cvtsd_f64( half ) );

	// Accumulator for the return value
	__m256d valVec = _mm256_setzero_pd();
	// Accumulator to increment gval scalar in the state
	__m256d gValVec = _mm256_setzero_pd();
	// Accumulator to store wval scalar in the state
	__m256d wValVec = _mm256_setzero_pd();

	// The main loop now leverages AVX to process 4 numbers per iteration
	for( size_t j = jBegin; j < jEnd; j += 4 )
	{
		const __m256i loadMask = remainderLoadMask( j, jEnd );
		const bool incompleteVector = ( j + 4 ) > jEnd;
		/* first, compute xcur[j] for y.  Because this objective is
		   separable, we can minimize over x analytically, and the minimum
		   dx is given by the solution of a quadratic equation:
				   u dx^2 + 2 v sigma^2 dx + u sigma^2 = 0
		   where u and v are defined by the sums below.  Because of
		   the definitions, it is guaranteed that |u/v| <= sigma,
		   and it follows that the only dx solution with |dx| <= sigma
		   is given by:
				   (v/u) sigma^2 (-1 + sqrt(1 - (u / v sigma)^2))
			   = (u/v) / (-1 - sqrt(1 - (u / v sigma)^2))
			   (which goes to zero as u -> 0).  The latter expression
		   is less susceptible to roundoff error. */

		   // Masked loads are very cheap on modern CPUs, same performance as a regular full-vector load
		   // Using _mm256_maskload_pd in every place which needs to load something[ j ]
		const __m256d x_j = _mm256_maskload_pd( &x[ j ], loadMask );

		const __m256d sigma_j = _mm256_maskload_pd( &sigma[ j ], loadMask );
		// Compare for sigma[ j ] != 0
		__m256d tmp = _mm256_cmp_pd( sigma_j, _mm256_setzero_pd(), _CMP_NEQ_OQ );
		// Exclude masked out elements at the end of the input
		const __m256d updateMask = _mm256_and_pd( tmp, _mm256_castsi256_pd( loadMask ) );

		if( _mm256_testz_pd( updateMask, updateMask ) )
		{
			// All active lanes compared false for sigma[ j ] != 0, can skip the complete 4 element batch

			// Store xcur[ j ] = x[ j ]
			// Unlike loads, masked stores are very expensive
			// Only using _mm256_maskstore_pd instruction for the last incomplete batch
			if( !incompleteVector )
				_mm256_storeu_pd( &xcur[ j ], x_j );
			else
				_mm256_maskstore_pd( &xcur[ j ], loadMask, x_j );
			continue;
		}

		// double u = dfdx[ j ];
		const __m256d dfdx_j = _mm256_maskload_pd( &dfdx[ j ], loadMask );
		__m256d u = dfdx_j;

		// double v = fabs( dfdx[ j ] ) * sigma[ j ] + 0.5 * rho;
		__m256d v = _mm256_fmadd_pd( vectorAbs( dfdx_j ), sigma_j, halfRho );

		const double* rsi = dfcdx + j;
#pragma loop( no_vector )
		for( size_t i = 0; i < m; i++, rsi += n )
			if( isNotNan( fcval[ i ] ) )
			{
				// dfcdx[ i * n + j ];
				assert( rsi == &dfcdx[ i * n + j ] );
				tmp = _mm256_maskload_pd( rsi, loadMask );
				const __m256d y_i = _mm256_broadcast_sd( &y[ i ] );
				// u += dfcdx[ i * n + j ] * y[ i ];
				u = _mm256_fmadd_pd( tmp, y_i, u );

				// v += ( fabs( dfcdx[ i * n + j ] ) * sigma[ j ] + 0.5 * rhoc[ i ] ) * y[ i ];
				const __m256d rhoc_i = _mm256_broadcast_sd( &rhoc[ i ] );
				tmp = vectorAbs( tmp );
				tmp = _mm256_mul_pd( tmp, sigma_j );
				tmp = _mm256_fmadd_pd( rhoc_i, half, tmp );
				v = _mm256_fmadd_pd( tmp, y_i, v );
			}

		// const double sigma2 = sqr( sigma[ j ] );
		const __m256d sigma2 = vectorSquare( sigma_j );
		// u *= sigma2;
		u = _mm256_mul_pd( u, sigma2 );

		// double dx = ( u / v ) / ( -1 - sqrt( fabs( 1 - sqr( u / ( v * sigma[ j ] ) ) ) ) );
		tmp = _mm256_mul_pd( v, sigma_j );	// v * sigma[ j ]
		tmp = _mm256_div_pd( u, tmp );	// u / ( v * sigma[ j ] )
		tmp = _mm256_fnmadd_pd( tmp, tmp, one ); // 1 - sqr( u / ( v * sigma[ j ] ) )
		tmp = vectorAbs( tmp ); // fabs( 1 - sqr( u / ( v * sigma[ j ] ) ) )
		tmp = _mm256_sqrt_pd( tmp ); // sqrt( fabs( 1 - sqr( u / ( v * sigma[ j ] ) ) ) ) );
		tmp = _mm256_add_pd( tmp, one ); // 1 + sqrt( fabs( 1 - sqr( u / ( v * sigma[ j ] ) ) ) )
		tmp = vectorNegate( tmp ); // -1 - sqrt( fabs( 1 - sqr( u / ( v * sigma[ j ] ) ) ) )
		tmp = _mm256_mul_pd( tmp, v );
		__m256d dx = _mm256_div_pd( u, tmp );

		// xcur[ j ] = x[ j ] + dx;
		__m256d xcurr_j = _mm256_add_pd( x_j, dx );

		// First pass of the clamping, into lb[ j ] .. ub[ j ]
		xcurr_j = _mm256_max_pd( xcurr_j, _mm256_maskload_pd( &lb[ j ], loadMask ) );
		xcurr_j = _mm256_min_pd( xcurr_j, _mm256_maskload_pd( &ub[ j ], loadMask ) );

		// Second pass of clamping, into x[ j ] - 0.9 * sigma[ j ] .. x[ j ] + 0.9 * sigma[ j ]
		{
			const __m256d lower = _mm256_fnmadd_pd( sigma_j, pointNine, x_j );
			const __m256d upper = _mm256_fmadd_pd( sigma_j, pointNine, x_j );
			xcurr_j = _mm256_max_pd( xcurr_j, lower );
			xcurr_j = _mm256_min_pd( xcurr_j, upper );
		}

		// Storing the following numbers to the xcur slice of memory: ( updateMask ? xcurr_j : x_j )
		// This enables full-vector stores except the last incomplete batch
		tmp = _mm256_blendv_pd( x_j, xcurr_j, updateMask );
		if( !incompleteVector )
			_mm256_storeu_pd( &xcur[ j ], tmp );
		else
			_mm256_maskstore_pd( &xcur[ j ], loadMask, tmp );

		// dx = xcur[ j ] - x[ j ];
		dx = _mm256_sub_pd( xcurr_j, x_j );

		/* function value: */
		// const double dx2 = dx * dx;
		const __m256d dx2 = vectorSquare( dx );
		// const double denominv = 1.0 / ( sigma2 - dx2 );
		const __m256d denominv = _mm256_div_pd( one, _mm256_sub_pd( sigma2, dx2 ) );
		// val += ( u * dx + v * dx2 ) * denominv
		// We postpone horizontal reduction until after the loop
		tmp = _mm256_mul_pd( u, dx );
		tmp = _mm256_fmadd_pd( v, dx2, tmp );
		// We want to skip the inactive ones, FMA with multiplier 0 doesn't change the result
		// denominv was computed as 1.0 / something which will probably be NAN for inactive lanes, that's why masking that particular argument instead of `tmp`
		valVec = _mm256_fmadd_pd( tmp, _mm256_and_pd( denominv, updateMask ), valVec );

		/* update gval, wval, gcval (approximant functions) */

		// const double c = sigma2 * dx;
		const __m256d c = _mm256_mul_pd( sigma2, dx );
		// d->gval += ( dfdx[ j ] * c + ( fabs( dfdx[ j ] ) * sigma[ j ] + 0.5 * rho ) * dx2 ) * denominv;
		tmp = _mm256_fmadd_pd( vectorAbs( dfdx_j ), sigma_j, halfRho ); // fabs( dfdx[ j ] ) * sigma[ j ] + 0.5 * rho 
		tmp = _mm256_mul_pd( tmp, dx2 ); // ( fabs( dfdx[ j ] ) * sigma[ j ] + 0.5 * rho ) * dx2
		tmp = _mm256_fmadd_pd( dfdx_j, c, tmp ); // dfdx[ j ] * c + ( fabs( dfdx[ j ] ) * sigma[ j ] + 0.5 * rho ) * dx2
		tmp = _mm256_mul_pd( tmp, denominv );
		// Mask away inactive lanes
		tmp = _mm256_and_pd( tmp, updateMask );
		// Postponing horizontal reduction after the loop
		gValVec = _mm256_add_pd( gValVec, tmp );

		// d->wval += 0.5 * dx2 * denominv;
		tmp = _mm256_mul_pd( dx2, denominv );
		// Mask away inactive lanes
		tmp = _mm256_and_pd( tmp, updateMask );
		// Postponing horizontal reduction, and `0.5` multiplication, after the loop
		wValVec = _mm256_add_pd( wValVec, tmp );

		rsi = dfcdx + j;
#pragma loop( no_vector )
		for( size_t i = 0; i < m; i++, rsi += n )
			if( isNotNan( fcval[ i ] ) )
			{
				// dfcdx[ i * n + j ];
				assert( rsi == &dfcdx[ i * n + j ] );
				const __m256d dfcdx_j = _mm256_maskload_pd( rsi, loadMask );
				const __m256d rhoc_i = _mm256_broadcast_sd( &rhoc[ i ] );

				// gcval[ i ] += ( tmp * c + ( fastAbs( tmp ) * sigma[ j ] + 0.5 * rhoc[ i ] ) * dx2 ) * denominv;
				tmp = vectorAbs( dfcdx_j );
				tmp = _mm256_mul_pd( tmp, sigma_j ); // fastAbs( tmp ) * sigma[ j ]
				tmp = _mm256_fmadd_pd( rhoc_i, half, tmp ); // fastAbs( tmp ) * sigma[ j ] + 0.5 * rhoc[ i ]
				tmp = _mm256_mul_pd( tmp, dx2 );
				tmp = _mm256_fmadd_pd( dfcdx_j, c, tmp ); // ( tmp * c + ( fastAbs( tmp ) * sigma[ j ] + 0.5 * rhoc[ i ] ) * dx2 )
				tmp = _mm256_mul_pd( tmp, denominv );
				// Mask away inactive lanes, and increment the number in memory
				tmp = _mm256_and_pd( tmp, updateMask );
				// Can't postpone horizontal reduction because inner loop of the dynamic length
				gcval[ i ] += horizontalSum( tmp );
			}
	}

	// Update thread local accumulator vectors
	// They are in the fist 12 elements of the reduction buffer, 4 elements per vector
	valVec = _mm256_add_pd( valVec, _mm256_load_pd( reductionBuffer ) );
	gValVec = _mm256_add_pd( gValVec, _mm256_load_pd( reductionBuffer + 4 ) );
	wValVec = _mm256_add_pd( wValVec, _mm256_load_pd( reductionBuffer + 8 ) );

	// Store the updates back to memory; that block of memory is private to the current thread, no concurrency shenanigans
	_mm256_store_pd( reductionBuffer, valVec );
	_mm256_store_pd( reductionBuffer + 4, gValVec );
	_mm256_store_pd( reductionBuffer + 8, wValVec );
}

namespace
{
	// rdi[ 0 .. length - 1 ] += rsi[ 0 .. length - 1 ]
	inline void addDoubles( double* rdi, const size_t length, const double* rsi )
	{
		// Round the length down to a multiple of 4
		const size_t lengthAligned = length & (ptrdiff_t)-4;
		const double* const rsiEndAligned = rsi + lengthAligned;

		// Handle full vectors in the loop
		for( ; rsi < rsiEndAligned; rdi += 4 )
		{
			__m256d acc = _mm256_loadu_pd( rdi );
			const __m256d inc = _mm256_loadu_pd( rsi );
			rsi += 4;
			acc = _mm256_add_pd( acc, inc );
			_mm256_storeu_pd( rdi, acc );
		}

		// Handle the remainder
		const size_t rem = length % 4;
		if( 0 != rem )
		{
			const __m256i loadMask = remainderLoadMask( 0, rem );

			__m256d acc = _mm256_maskload_pd( rdi, loadMask );
			const __m256d inc = _mm256_maskload_pd( rsi, loadMask );
			acc = _mm256_add_pd( acc, inc );
			_mm256_maskstore_pd( rdi, loadMask, acc );
		}
	}
}

// Add FP64 buffers produced by the threads of the reducer
// The total sum is in the initial slice of the buffer
static void reduceThreads( const ParallelReduce<dual_data>& pr )
{
	const size_t lengthEntry = pr.threadBufferSize();
	double* const basePointer = pr.buffer();

	// That parallel reduction class pads and aligns thread buffers by cache lines
	// This means we don't need masked loads or stores, and can do 2 AVX vectors per iteration of the inner loop
	assert( 0 == lengthEntry % 8 );
	assert( 0 == ( (size_t)basePointer ) % 64 );

	size_t buffersLeft = pr.countWorkers();
	while( buffersLeft > 1 )
	{
		// Length of the smaller half at the tail
		const size_t length = buffersLeft / 2;
		// Length of the larger half at the head
		const size_t sourceOffset = buffersLeft - length;
		// Set remaining size to the first, larger half
		buffersLeft = sourceOffset;

		// Add two slices of FP64 numbers in memory, of size length * lengthEntry
		// The accumulator slice is at the start of the reduction buffer
		// The second slice being added starts at the offset `sourceOffset * lengthEntry`
		const double* rsi = basePointer + ( sourceOffset * lengthEntry );
		const double* const rsiEnd = basePointer + ( ( sourceOffset + length ) * lengthEntry );

		for( double* rdi = basePointer; rsi < rsiEnd; rdi += 8 )
		{
			__m256d a = _mm256_load_pd( rdi );
			__m256d b = _mm256_load_pd( rdi + 4 );

			a = _mm256_add_pd( a, _mm256_load_pd( rsi ) );
			b = _mm256_add_pd( b, _mm256_load_pd( rsi + 4 ) );
			rsi += 8;

			_mm256_store_pd( rdi, a );
			_mm256_store_pd( rdi + 4, b );
		}
	}
}
#endif

static double dual_func( unsigned m32, const double* y, double* grad, void* d_ )
{
	const size_t m = m32;
	dual_data* const d = (dual_data*)d_;
	const size_t n = d->n;
	const double* x = d->x;
	const double* lb = d->lb;
	const double* ub = d->ub;
	const double* sigma = d->sigma;
	const double* dfdx = d->dfdx;
	const double* dfcdx = d->dfcdx;
	const double* rhoc = d->rhoc;
	const double* fcval = d->fcval;

	double* xcur = d->xcur;
	double* gcval = d->gcval;
	size_t i;
	double val;

	d->count++;

	val = d->gval = d->fval;
	// Removed `d->wval = 0` because replacing the number at the end instead of incrementing

	// Accumulator for the return value
	__m256d valVec = _mm256_setzero_pd();
	for( i = 0; i < m; i += 4 )
	{
		const __m256i loadMask = remainderLoadMask( i, m );
		const bool incompleteVector = ( i + 4 ) > m;

		// val += y[ i ] * ( gcval[ i ] = nlopt_isnan( fcval[ i ] ) ? 0 : fcval[ i ] );
		__m256d fcval_i = _mm256_maskload_pd( &fcval[ i ], loadMask );
		const __m256d y_i = _mm256_maskload_pd( &y[ i ], loadMask );

		fcval_i = zeroFromNan( fcval_i );
		if( !incompleteVector )
			_mm256_storeu_pd( &gcval[ i ], fcval_i );
		else
			_mm256_maskstore_pd( &gcval[ i ], loadMask, fcval_i );

		valVec = _mm256_fmadd_pd( y_i, fcval_i, valVec );
	}

#if USE_PARALLEL_FOR
	d->m = m;
	d->y = y;

	if( !d->parallelReduce.allocateBuffer( 12 + m ) )
		throw std::bad_alloc();

	// Zero out the reduction buffers for all threads
	d->parallelReduce.zeroMemory();
	// Dispatch parallel work to the thread pool, wait for completion
	d->parallelReduce.dispatch( &poolCallback, d, d->countJobs );
	// Sum the output buffers produced by all threads on the pool
	reduceThreads( d->parallelReduce );

	const double* const buffer = d->parallelReduce.buffer();
	// Increment `gcval` accumulators
	if( m != 0 )
		addDoubles( gcval, m, buffer + 12 );

	// Load val, gVal, wVal vectors from the initial 12 elements of the output buffer
	valVec = _mm256_add_pd( valVec, _mm256_load_pd( buffer ) );
	const __m256d gValVec = _mm256_load_pd( buffer + 4 );
	const __m256d wValVec = _mm256_load_pd( buffer + 8 );
#else
	// Create some constant vectors
	const __m256d half = _mm256_set1_pd( 0.5 );
	const __m256d one = _mm256_set1_pd( 1 );
	const __m256d pointNine = _mm256_set1_pd( 0.9 );
	const __m256d halfRho = _mm256_set1_pd( d->rho * _mm256_cvtsd_f64( half ) );

	// Accumulator to increment gval scalar in the state
	__m256d gValVec = _mm256_setzero_pd();
	// Accumulator to store wval scalar in the state
	__m256d wValVec = _mm256_setzero_pd();

	// The main loop now leverages AVX to process 4 numbers per iteration
	for( size_t j = 0; j < n; j += 4 )
	{
		const __m256i loadMask = remainderLoadMask( j, n );
		const bool incompleteVector = ( j + 4 ) > n;
		/* first, compute xcur[j] for y.  Because this objective is
		   separable, we can minimize over x analytically, and the minimum
		   dx is given by the solution of a quadratic equation:
				   u dx^2 + 2 v sigma^2 dx + u sigma^2 = 0
		   where u and v are defined by the sums below.  Because of
		   the definitions, it is guaranteed that |u/v| <= sigma,
		   and it follows that the only dx solution with |dx| <= sigma
		   is given by:
				   (v/u) sigma^2 (-1 + sqrt(1 - (u / v sigma)^2))
			   = (u/v) / (-1 - sqrt(1 - (u / v sigma)^2))
			   (which goes to zero as u -> 0).  The latter expression
		   is less susceptible to roundoff error. */

		   // Masked loads are very cheap on modern CPUs, same performance as a regular full-vector load
		   // Using _mm256_maskload_pd in every place which needs to load something[ j ]
		const __m256d x_j = _mm256_maskload_pd( &x[ j ], loadMask );

		const __m256d sigma_j = _mm256_maskload_pd( &sigma[ j ], loadMask );
		// Compare for sigma[ j ] != 0
		__m256d tmp = _mm256_cmp_pd( sigma_j, _mm256_setzero_pd(), _CMP_NEQ_OQ );
		// Exclude masked out elements at the end of the input
		const __m256d updateMask = _mm256_and_pd( tmp, _mm256_castsi256_pd( loadMask ) );

		if( _mm256_testz_pd( updateMask, updateMask ) )
		{
			// All active lanes compared false for sigma[ j ] != 0, can skip the complete 4 element batch

			// Store xcur[ j ] = x[ j ]
			// Unlike loads, masked stores are very expensive
			// Only using _mm256_maskstore_pd instruction for the last incomplete batch
			if( !incompleteVector )
				_mm256_storeu_pd( &xcur[ j ], x_j );
			else
				_mm256_maskstore_pd( &xcur[ j ], loadMask, x_j );
			continue;
		}

		// double u = dfdx[ j ];
		const __m256d dfdx_j = _mm256_maskload_pd( &dfdx[ j ], loadMask );
		__m256d u = dfdx_j;

		// double v = fabs( dfdx[ j ] ) * sigma[ j ] + 0.5 * rho;
		__m256d v = _mm256_fmadd_pd( vectorAbs( dfdx_j ), sigma_j, halfRho );

		const double* rsi = dfcdx + j;
#pragma loop( no_vector )
		for( i = 0; i < m; i++, rsi += n )
			if( isNotNan( fcval[ i ] ) )
			{
				// dfcdx[ i * n + j ];
				assert( rsi == &dfcdx[ i * n + j ] );
				tmp = _mm256_maskload_pd( rsi, loadMask );
				const __m256d y_i = _mm256_broadcast_sd( &y[ i ] );
				// u += dfcdx[ i * n + j ] * y[ i ];
				u = _mm256_fmadd_pd( tmp, y_i, u );

				// v += ( fabs( dfcdx[ i * n + j ] ) * sigma[ j ] + 0.5 * rhoc[ i ] ) * y[ i ];
				const __m256d rhoc_i = _mm256_broadcast_sd( &rhoc[ i ] );
				tmp = vectorAbs( tmp );
				tmp = _mm256_mul_pd( tmp, sigma_j );
				tmp = _mm256_fmadd_pd( rhoc_i, half, tmp );
				v = _mm256_fmadd_pd( tmp, y_i, v );
			}

		// const double sigma2 = sqr( sigma[ j ] );
		const __m256d sigma2 = vectorSquare( sigma_j );
		// u *= sigma2;
		u = _mm256_mul_pd( u, sigma2 );

		// double dx = ( u / v ) / ( -1 - sqrt( fabs( 1 - sqr( u / ( v * sigma[ j ] ) ) ) ) );
		tmp = _mm256_mul_pd( v, sigma_j );	// v * sigma[ j ]
		tmp = _mm256_div_pd( u, tmp );	// u / ( v * sigma[ j ] )
		tmp = _mm256_fnmadd_pd( tmp, tmp, one ); // 1 - sqr( u / ( v * sigma[ j ] ) )
		tmp = vectorAbs( tmp ); // fabs( 1 - sqr( u / ( v * sigma[ j ] ) ) )
		tmp = _mm256_sqrt_pd( tmp ); // sqrt( fabs( 1 - sqr( u / ( v * sigma[ j ] ) ) ) ) );
		tmp = _mm256_add_pd( tmp, one ); // 1 + sqrt( fabs( 1 - sqr( u / ( v * sigma[ j ] ) ) ) )
		tmp = vectorNegate( tmp ); // -1 - sqrt( fabs( 1 - sqr( u / ( v * sigma[ j ] ) ) ) )
		tmp = _mm256_mul_pd( tmp, v );
		__m256d dx = _mm256_div_pd( u, tmp );

		// xcur[ j ] = x[ j ] + dx;
		__m256d xcurr_j = _mm256_add_pd( x_j, dx );

		// First pass of the clamping, into lb[ j ] .. ub[ j ]
		xcurr_j = _mm256_max_pd( xcurr_j, _mm256_maskload_pd( &lb[ j ], loadMask ) );
		xcurr_j = _mm256_min_pd( xcurr_j, _mm256_maskload_pd( &ub[ j ], loadMask ) );

		// Second pass of clamping, into x[ j ] - 0.9 * sigma[ j ] .. x[ j ] + 0.9 * sigma[ j ]
		{
			const __m256d lower = _mm256_fnmadd_pd( sigma_j, pointNine, x_j );
			const __m256d upper = _mm256_fmadd_pd( sigma_j, pointNine, x_j );
			xcurr_j = _mm256_max_pd( xcurr_j, lower );
			xcurr_j = _mm256_min_pd( xcurr_j, upper );
		}

		// Storing the following numbers to the xcur slice of memory: ( updateMask ? xcurr_j : x_j )
		// This enables full-vector stores except the last incomplete batch
		tmp = _mm256_blendv_pd( x_j, xcurr_j, updateMask );
		if( !incompleteVector )
			_mm256_storeu_pd( &xcur[ j ], tmp );
		else
			_mm256_maskstore_pd( &xcur[ j ], loadMask, tmp );

		// dx = xcur[ j ] - x[ j ];
		dx = _mm256_sub_pd( xcurr_j, x_j );

		/* function value: */
		// const double dx2 = dx * dx;
		const __m256d dx2 = vectorSquare( dx );
		// const double denominv = 1.0 / ( sigma2 - dx2 );
		const __m256d denominv = _mm256_div_pd( one, _mm256_sub_pd( sigma2, dx2 ) );
		// val += ( u * dx + v * dx2 ) * denominv
		// We postpone horizontal reduction until after the loop
		tmp = _mm256_mul_pd( u, dx );
		tmp = _mm256_fmadd_pd( v, dx2, tmp );
		// We want to skip the inactive ones, FMA with multiplier 0 doesn't change the result
		// denominv was computed as 1.0 / something which will probably be NAN for inactive lanes, that's why masking that particular argument instead of `tmp`
		valVec = _mm256_fmadd_pd( tmp, _mm256_and_pd( denominv, updateMask ), valVec );

		/* update gval, wval, gcval (approximant functions) */

		// const double c = sigma2 * dx;
		const __m256d c = _mm256_mul_pd( sigma2, dx );
		// d->gval += ( dfdx[ j ] * c + ( fabs( dfdx[ j ] ) * sigma[ j ] + 0.5 * rho ) * dx2 ) * denominv;
		tmp = _mm256_fmadd_pd( vectorAbs( dfdx_j ), sigma_j, halfRho ); // fabs( dfdx[ j ] ) * sigma[ j ] + 0.5 * rho 
		tmp = _mm256_mul_pd( tmp, dx2 ); // ( fabs( dfdx[ j ] ) * sigma[ j ] + 0.5 * rho ) * dx2
		tmp = _mm256_fmadd_pd( dfdx_j, c, tmp ); // dfdx[ j ] * c + ( fabs( dfdx[ j ] ) * sigma[ j ] + 0.5 * rho ) * dx2
		tmp = _mm256_mul_pd( tmp, denominv );
		// Mask away inactive lanes
		tmp = _mm256_and_pd( tmp, updateMask );
		// Postponing horizontal reduction after the loop
		gValVec = _mm256_add_pd( gValVec, tmp );

		// d->wval += 0.5 * dx2 * denominv;
		tmp = _mm256_mul_pd( dx2, denominv );
		// Mask away inactive lanes
		tmp = _mm256_and_pd( tmp, updateMask );
		// Postponing horizontal reduction, and `0.5` multiplication, after the loop
		wValVec = _mm256_add_pd( wValVec, tmp );

		rsi = dfcdx + j;
#pragma loop( no_vector )
		for( i = 0; i < m; i++, rsi += n )
			if( isNotNan( fcval[ i ] ) )
			{
				// dfcdx[ i * n + j ];
				assert( rsi == &dfcdx[ i * n + j ] );
				const __m256d dfcdx_j = _mm256_maskload_pd( rsi, loadMask );
				const __m256d rhoc_i = _mm256_broadcast_sd( &rhoc[ i ] );

				// gcval[ i ] += ( tmp * c + ( fastAbs( tmp ) * sigma[ j ] + 0.5 * rhoc[ i ] ) * dx2 ) * denominv;
				tmp = vectorAbs( dfcdx_j );
				tmp = _mm256_mul_pd( tmp, sigma_j ); // fastAbs( tmp ) * sigma[ j ]
				tmp = _mm256_fmadd_pd( rhoc_i, half, tmp ); // fastAbs( tmp ) * sigma[ j ] + 0.5 * rhoc[ i ]
				tmp = _mm256_mul_pd( tmp, dx2 );
				tmp = _mm256_fmadd_pd( dfcdx_j, c, tmp ); // ( tmp * c + ( fastAbs( tmp ) * sigma[ j ] + 0.5 * rhoc[ i ] ) * dx2 )
				tmp = _mm256_mul_pd( tmp, denominv );
				// Mask away inactive lanes, and increment the number in memory
				tmp = _mm256_and_pd( tmp, updateMask );
				// Can't postpone horizontal reduction because inner loop of the dynamic length
				gcval[ i ] += horizontalSum( tmp );
			}
	}
#endif	// !USE_PARALLEL_FOR

	// Apply postponed horizontal reductions and memory stores
	val += horizontalSum( valVec );
	d->gval += horizontalSum( gValVec );
	// Replacing the number instead of incrementing; also multiplying by 0.5 because removed that multiplication from the main loop
	d->wval = horizontalSum( wValVec ) * 0.5;

	/* gradient is easy to compute: since we are at a minimum x (dval/dx=0), we only need the partial derivative with respect to y, and we negate because we are maximizing: */
	if( nullptr != grad )
		negateDoubles( grad, m, gcval );

	return -val;
}

/***********************************************************************/

/* note that we implement a hidden feature not in the standard
   nlopt_minimize_constrained interface: whenever the constraint
   function returns NaN, that constraint becomes inactive. */

nlopt_result mma_minimize( unsigned n, nlopt_func f, void* f_data,
	unsigned m, nlopt_constraint* fc,
	const double* lb, const double* ub, /* bounds */
	double* x, /* in: initial guess, out: minimizer */
	double* minf,
	nlopt_stopping* stop,
	nlopt_opt dual_opt, int inner_maxeval, unsigned verbose, double rho_init,
	const double* sigma_init )
{
	nlopt_result ret = NLOPT_SUCCESS;
	double* xcur, rho, * sigma, * dfdx, * dfdx_cur, * xprev, * xprevprev, fcur;
	double* dfcdx, * dfcdx_cur;
	double* fcval, * fcval_cur, * rhoc, * gcval, * y, * dual_lb, * dual_ub;
	unsigned i, ifc, j, k = 0;
	dual_data dd;
	int feasible;
	double infeasibility;
	unsigned mfc;

	verbose = MAX( mma_verbose, verbose );

	m = nlopt_count_constraints( mfc = m, fc );
	if( nlopt_get_dimension( dual_opt ) != m ) {
		nlopt_stop_msg( stop, "dual optimizer has wrong dimension %d != %d",
			nlopt_get_dimension( dual_opt ), m );
		return NLOPT_INVALID_ARGS;
	}

#if USE_PARALLEL_FOR
	if( !dd.parallelReduce.allocateBuffer( 12 + m ) )
		return NLOPT_OUT_OF_MEMORY;
	dd.computeJobSize( n );
#endif

	sigma = (double*)malloc( sizeof( double ) * ( 6 * n + 2 * m * n + m * 7 ) );
	if( !sigma ) return NLOPT_OUT_OF_MEMORY;
	dfdx = sigma + n;
	dfdx_cur = dfdx + n;
	xcur = dfdx_cur + n;
	xprev = xcur + n;
	xprevprev = xprev + n;
	fcval = xprevprev + n;
	fcval_cur = fcval + m;
	rhoc = fcval_cur + m;
	gcval = rhoc + m;
	dual_lb = gcval + m;
	dual_ub = dual_lb + m;
	y = dual_ub + m;
	dfcdx = y + m;
	dfcdx_cur = dfcdx + m * n;

	dd.n = n;
	dd.x = x;
	dd.lb = lb;
	dd.ub = ub;
	dd.sigma = sigma;
	dd.dfdx = dfdx;
	dd.dfcdx = dfcdx;
	dd.fcval = fcval;
	dd.rhoc = rhoc;
	dd.xcur = xcur;
	dd.gcval = gcval;

	for( j = 0; j < n; ++j ) {
		if( sigma_init && sigma_init[ j ] > 0 )
			sigma[ j ] = sigma_init[ j ];
		else if( nlopt_isinf( ub[ j ] ) || nlopt_isinf( lb[ j ] ) )
			sigma[ j ] = 1.0; /* arbitrary default */
		else
			sigma[ j ] = 0.5 * ( ub[ j ] - lb[ j ] );
	}
	rho = rho_init;
	for( i = 0; i < m; ++i ) {
		rhoc[ i ] = rho_init;
		dual_lb[ i ] = y[ i ] = 0.0;
		dual_ub[ i ] = HUGE_VAL;
	}

	dd.fval = fcur = *minf = f( n, x, dfdx, f_data );
	++*( stop->nevals_p );
	memcpy( xcur, x, sizeof( double ) * n );
	if( nlopt_stop_forced( stop ) ) { ret = NLOPT_FORCED_STOP; goto done; }

	feasible = 1; infeasibility = 0;
	for( i = ifc = 0; ifc < mfc; ++ifc ) {
		nlopt_eval_constraint( fcval + i, dfcdx + i * n,
			fc + ifc, n, x );
		i += fc[ ifc ].m;
		if( nlopt_stop_forced( stop ) ) { ret = NLOPT_FORCED_STOP; goto done; }
	}
	for( i = 0; i < m; ++i ) {
		feasible = feasible && ( fcval[ i ] <= 0 || nlopt_isnan( fcval[ i ] ) );
		if( fcval[ i ] > infeasibility ) infeasibility = fcval[ i ];
	}
	/* For non-feasible initial points, set a finite (large)
   upper-bound on the dual variables.  What this means is that,
   if no feasible solution is found from the dual problem, it
   will minimize the dual objective with the unfeasible
   constraint weighted by 1e40 -- basically, minimizing the
   unfeasible constraint until it becomes feasible or until we at
   least obtain a step towards a feasible point.

   Svanberg suggested a different approach in his 1987 paper, basically
   introducing additional penalty variables for unfeasible constraints,
   but this is easier to implement and at least as efficient. */
	if( !feasible )
		for( i = 0; i < m; ++i ) dual_ub[ i ] = 1e40;

	nlopt_set_min_objective( dual_opt, dual_func, &dd );
	nlopt_set_lower_bounds( dual_opt, dual_lb );
	nlopt_set_upper_bounds( dual_opt, dual_ub );
	nlopt_set_stopval( dual_opt, -HUGE_VAL );
	nlopt_remove_inequality_constraints( dual_opt );
	nlopt_remove_equality_constraints( dual_opt );

	while( 1 ) { /* outer iterations */
		int inner_nevals = 0;
		double fprev = fcur;
		if( nlopt_stop_forced( stop ) ) ret = NLOPT_FORCED_STOP;
		else if( nlopt_stop_evals( stop ) ) ret = NLOPT_MAXEVAL_REACHED;
		else if( nlopt_stop_time( stop ) ) ret = NLOPT_MAXTIME_REACHED;
		else if( feasible && *minf < stop->minf_max )
			ret = NLOPT_MINF_MAX_REACHED;
		if( ret != NLOPT_SUCCESS ) goto done;
		if( ++k > 1 ) memcpy( xprevprev, xprev, sizeof( double ) * n );
		memcpy( xprev, xcur, sizeof( double ) * n );

		while( 1 ) { /* inner iterations */
			double min_dual, infeasibility_cur;
			int feasible_cur, inner_done;
			unsigned save_verbose;
			int new_infeasible_constraint;
			nlopt_result reti;

			/* solve dual problem */
			dd.rho = rho; dd.count = 0;
			save_verbose = mma_verbose;
			mma_verbose = 0; /* no recursive verbosity */
			reti = nlopt_optimize_limited( dual_opt, y, &min_dual,
				0,
				stop->maxtime - ( nlopt_seconds()
					- stop->start ) );
			mma_verbose = save_verbose;
			if( reti < 0 || reti == NLOPT_MAXTIME_REACHED ) {
				ret = reti;
				goto done;
			}

			dual_func( m, y, NULL, &dd ); /* evaluate final xcur etc. */
			if( verbose ) {
				printf( "MMA dual converged in %d iterations to g=%g:\n",
					dd.count, dd.gval );
				for( i = 0; i < MIN( verbose, m ); ++i )
					printf( "    MMA y[%u]=%g, gc[%u]=%g\n",
						i, y[ i ], i, dd.gcval[ i ] );
			}

			fcur = f( n, xcur, dfdx_cur, f_data );
			++*( stop->nevals_p );
			++inner_nevals;
			if( nlopt_stop_forced( stop ) ) {
				ret = NLOPT_FORCED_STOP; goto done;
			}
			feasible_cur = 1; infeasibility_cur = 0;
			new_infeasible_constraint = 0;
			inner_done = dd.gval >= fcur;
			for( i = ifc = 0; ifc < mfc; ++ifc ) {
				nlopt_eval_constraint( fcval_cur + i, dfcdx_cur + i * n,
					fc + ifc, n, xcur );
				i += fc[ ifc ].m;
				if( nlopt_stop_forced( stop ) ) {
					ret = NLOPT_FORCED_STOP; goto done;
				}
			}
			for( i = ifc = 0; ifc < mfc; ++ifc ) {
				unsigned i0 = i, inext = i + fc[ ifc ].m;
				for( ; i < inext; ++i )
					if( !nlopt_isnan( fcval_cur[ i ] ) ) {
						feasible_cur = feasible_cur
							&& ( fcval_cur[ i ] <= fc[ ifc ].tol[ i - i0 ] );
						if( !nlopt_isnan( fcval[ i ] ) )
							inner_done = inner_done &&
							( dd.gcval[ i ] >= fcval_cur[ i ] );
						else if( fcval_cur[ i ] > 0 )
							new_infeasible_constraint = 1;
						if( fcval_cur[ i ] > infeasibility_cur )
							infeasibility_cur = fcval_cur[ i ];
					}
			}

			inner_done = inner_done || ( inner_maxeval > 0 && inner_nevals == inner_maxeval );

			if( ( fcur < *minf && ( inner_done || feasible_cur || !feasible ) )
				|| ( !feasible && infeasibility_cur < infeasibility ) ) {
				if( verbose && !feasible_cur )
					printf( "MMA - using infeasible point?\n" );
				dd.fval = *minf = fcur;
				infeasibility = infeasibility_cur;
				memcpy( fcval, fcval_cur, sizeof( double ) * m );
				memcpy( x, xcur, sizeof( double ) * n );
				memcpy( dfdx, dfdx_cur, sizeof( double ) * n );
				memcpy( dfcdx, dfcdx_cur, sizeof( double ) * n * m );

				/* once we have reached a feasible solution, the
				   algorithm should never make the solution infeasible
				   again (if inner_done), although the constraints may
				   be violated slightly by rounding errors etc. so we
				   must be a little careful about checking feasibility */
				if( infeasibility_cur == 0 ) {
					if( !feasible ) { /* reset upper bounds to infin. */
						for( i = 0; i < m; ++i ) dual_ub[ i ] = HUGE_VAL;
						nlopt_set_upper_bounds( dual_opt, dual_ub );
					}
					feasible = 1;
				}
				else if( new_infeasible_constraint ) feasible = 0;

			}
			if( nlopt_stop_forced( stop ) ) ret = NLOPT_FORCED_STOP;
			else if( nlopt_stop_evals( stop ) ) ret = NLOPT_MAXEVAL_REACHED;
			else if( nlopt_stop_time( stop ) ) ret = NLOPT_MAXTIME_REACHED;
			else if( feasible && *minf < stop->minf_max )
				ret = NLOPT_MINF_MAX_REACHED;
			if( ret != NLOPT_SUCCESS ) goto done;

			if( inner_done ) break;

			if( fcur > dd.gval )
				rho = MIN( 10 * rho, 1.1 * ( rho + ( fcur - dd.gval ) / dd.wval ) );
			for( i = 0; i < m; ++i )
				if( !nlopt_isnan( fcval_cur[ i ] ) && fcval_cur[ i ] > dd.gcval[ i ] )
					rhoc[ i ] =
					MIN( 10 * rhoc[ i ],
						1.1 * ( rhoc[ i ] + ( fcval_cur[ i ] - dd.gcval[ i ] )
							/ dd.wval ) );

			if( verbose )
				printf( "MMA inner iteration: rho -> %g\n", rho );
			for( i = 0; i < MIN( verbose, m ); ++i )
				printf( "                 MMA rhoc[%u] -> %g\n", i, rhoc[ i ] );
		}

		if( nlopt_stop_ftol( stop, fcur, fprev ) )
			ret = NLOPT_FTOL_REACHED;
		if( nlopt_stop_x( stop, xcur, xprev ) )
			ret = NLOPT_XTOL_REACHED;
		if( ret != NLOPT_SUCCESS ) goto done;

		/* update rho and sigma for iteration k+1 */
		rho = MAX( 0.1 * rho, MMA_RHOMIN );
		if( verbose )
			printf( "MMA outer iteration: rho -> %g\n", rho );
		for( i = 0; i < m; ++i )
			rhoc[ i ] = MAX( 0.1 * rhoc[ i ], MMA_RHOMIN );
		for( i = 0; i < MIN( verbose, m ); ++i )
			printf( "                 MMA rhoc[%u] -> %g\n", i, rhoc[ i ] );
		if( k > 1 ) {
			for( j = 0; j < n; ++j ) {
				double dx2 = ( xcur[ j ] - xprev[ j ] ) * ( xprev[ j ] - xprevprev[ j ] );
				double gam = dx2 < 0 ? 0.7 : ( dx2 > 0 ? 1.2 : 1 );
				sigma[ j ] *= gam;
				if( !nlopt_isinf( ub[ j ] ) && !nlopt_isinf( lb[ j ] ) ) {
					sigma[ j ] = MIN( sigma[ j ], 10 * ( ub[ j ] - lb[ j ] ) );
					sigma[ j ] = MAX( sigma[ j ], 0.01 * ( ub[ j ] - lb[ j ] ) );
				}
			}
			for( j = 0; j < MIN( verbose, n ); ++j )
				printf( "                 MMA sigma[%u] -> %g\n",
					j, sigma[ j ] );
		}
	}

done:
	free( sigma );
	return ret;
}